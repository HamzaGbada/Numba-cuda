{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numba-CUDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpkSRa3ifsV+lE6iQBk5EQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamzaGbada/Numba-cuda/blob/main/Numba_CUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrCKIp46FQbi"
      },
      "source": [
        "فَلَو أَنَّ ما أَسعى لِأَدنى مَعيشَةٍ **** كَفاني وَلَم أَطلُب قَليلٌ مِنَ المالِ \n",
        "\n",
        "وَلَكِنَّما أَسعى لِمَجدٍ مُؤَثَّلٍ **** وَقَد يُدرِكُ المَجدَ المُؤَثَّلَ أَمثالي\n",
        "\n",
        "امرؤ القيس"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmfN0wZv20D-"
      },
      "source": [
        "In this notebook, we will discuss how to do parallel computing on your GPU with CUDA using python!\n",
        "\n",
        "The [NVIDIA® CUDA® Toolkit](https://developer.nvidia.com/cuda-toolkit) provides a development environment for creating high performance GPU-accelerated applications.\n",
        "\n",
        "We will cover these sections:\n",
        "\n",
        "1.   Cuda kernel\n",
        "2.   Memory management\n",
        "3.   Device function\n",
        "4.   Threads and blocks configuration\n",
        "5.   Application in image processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gogkjj7C73pk"
      },
      "source": [
        "Before we start, let's take a look on what is [Numba](https://numba.pydata.org/).\n",
        "\n",
        "So Numba is a just-in-time compiler that translates python code into machine code using [LLVM](https://llvm.org/), which makes numerical alogrithms execution speed in Python comparable to C or FORTRAN algorithms.\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npat4iVPwCrk"
      },
      "source": [
        "# Cuda Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFUlg5qKwSU7"
      },
      "source": [
        "As a first we must check [CUDA programming terminology](https://numba.pydata.org/numba-doc/dev/cuda/overview.html#terminology), let's take a minimal example where we add 2 for each element of a vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-i9yyAe73U0"
      },
      "source": [
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def add_gpu(x, out):\n",
        "  idx = cuda.grid(1)\n",
        "  out[idx] = x[idx] + 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcmO9CYGxAOj"
      },
      "source": [
        "Let's discuss this code in some details:\n",
        "\n",
        "When writing CUDA kernel, we must declare the input and the output of the kernel as argment.\n",
        "But, What's `cuda.grid(1)`? \n",
        "\n",
        "`cuda.grid(1)` returns the unique index for the current thread in the whole grid. With N threads, idx will range from 0 to N. We will discuss thread indexing in more details in the section 4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBrlwsNT0LCN"
      },
      "source": [
        "Now, we setup our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Svpk8unvkts"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.arange(10,dtype=np.float32)\n",
        "\n",
        "# send input vector to the device\n",
        "d_a = cuda.to_device(a)\n",
        "\n",
        "# create output vector on the device\n",
        "d_out = cuda.device_array_like(d_a)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ABv6TE-Fbt"
      },
      "source": [
        "# we decide to use 2 blocks, each containing 5 threads for our vector \n",
        "nbr_block_per_grid = 2\n",
        "nbr_thread_per_block = 5\n",
        "add_gpu[nbr_block_per_grid, nbr_thread_per_block](d_a, d_out)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQtsaEMi_Noa",
        "outputId": "d63417b6-b1ba-4a4a-f87a-ea74b77396be"
      },
      "source": [
        "# now we get our output\n",
        "out = d_out.copy_to_host()\n",
        "out"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCzUYYRt0LJb"
      },
      "source": [
        "Le Voilà!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfTwxr0e_ll6"
      },
      "source": [
        "Let's compare it to a CPU function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyz8fjNr_kOU"
      },
      "source": [
        "def add_cpu(x):\n",
        "   for i in range(x.size):\n",
        "     x[i]+=2\n",
        "   return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a03ZKZkijdDg",
        "outputId": "06da590e-b614-413c-cb69-f528986abc6e"
      },
      "source": [
        "%timeit add_gpu[nbr_block_per_grid, nbr_thread_per_block](d_a, d_out)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 8.00 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 136 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H13awt22j09I",
        "outputId": "6a675f23-5c02-4504-d864-b50e1d1491a3"
      },
      "source": [
        "%timeit add_cpu(a)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 loops, best of 5: 24.9 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnFvo5Y1kHPc"
      },
      "source": [
        " Wait! What is that! The CPU version is about 6 time faster!\n",
        " \n",
        "Let's understand what is happening together. When running on the GPU, the following steps are performed :\n",
        "\n",
        "1. the input data (the array a ) is transferred to the GPU memory;\n",
        "the calculation of the square root is done in parallel on the GPU for all elements of a\n",
        "2. the resulting array is sent back to the host system.\n",
        "\n",
        "If the calculation is too simple, the time spent in calculation is negligible compared to the data transfer time. \n",
        "\n",
        "\n",
        "Let's see what happens with a more involved calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx6dPpdtKGcp",
        "outputId": "4356779c-bed6-4a79-8636-c412fbc0c47f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.arange(32768,dtype=np.float32)\n",
        "\n",
        "# send input vector to the device\n",
        "d_a = cuda.to_device(a)\n",
        "\n",
        "# create output vector on the device\n",
        "d_out = cuda.device_array_like(d_a)\n",
        "\n",
        "# we decide to use 2 blocks, each containing 5 threads for our vector \n",
        "nbr_block_per_grid = 32\n",
        "nbr_thread_per_block = 1024\n",
        "add_gpu[nbr_block_per_grid, nbr_thread_per_block](d_a, d_out)\n",
        "\n",
        "%timeit add_gpu[nbr_block_per_grid, nbr_thread_per_block](d_a, d_out)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 loops, best of 5: 145 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJddsn4sKI0t",
        "outputId": "06f7f106-5af1-4e73-c095-b1680b99f9f5"
      },
      "source": [
        "%timeit add_cpu(a)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 81.1 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8johseuLCXY"
      },
      "source": [
        "Now, it's clear! when we increase the size of our vector, the calculation became more complex and so the GPU version became faster!\n",
        "\n",
        "**Conclusion:** When dealing with simple data and calculation, it's a waste of time to use GPU.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty2V7d1Vx43t"
      },
      "source": [
        "# Memory management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtJ4epUCzx-4"
      },
      "source": [
        "In the last section, we have seen a little example of memory management without any explication. So the main idea of this section is to learn how to deal with data transfers between ***Host*** and ***Device***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoMVVIeW22UO"
      },
      "source": [
        "It is advisable to minimize data transfers to improve performance and this can be done with device arrays. \n",
        "\n",
        "To show this in simple way, we will use an example of normalizing pixel range of an image from [0,255] into [0,10], our calculation will be based on this formula: \n",
        "\n",
        "$j =  10 \\times \\frac{i}{255} \\;\\;\\; \\forall 0 \\leq i\\leq 255 $\n",
        "\n",
        "where i is the intensity of the initiale image and j is the output.\n",
        "\n",
        "After that, we will perfom a Gamma Correction based on this function:\n",
        "\n",
        "$f(V)=V^\\alpha \\: \\: \\: \\; \\alpha>0$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7yHIL5Rgoz"
      },
      "source": [
        "Before we start let's see how they operate in CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDuyxztZKhQ0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "n = 512\n",
        "\n",
        "# The image will be generated randomly as numpy array\n",
        "image = np.random.randint(256, size=(n, n))\n",
        "\n",
        "def normlize_image(image):\n",
        "  return image*10 / 255\n",
        "\n",
        "def gamma_correction(image, gamma):\n",
        "  image = np.power(image / float(10), gamma)\n",
        "  return image * 10"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDZgtbnVS3y6",
        "outputId": "49006eb1-1c36-4dc7-f1f5-be9b7cd5acaa"
      },
      "source": [
        "%%timeit\n",
        "gamma = 0.5\n",
        "\n",
        "norm_img = normlize_image(image)\n",
        "\n",
        "gamma_corr = gamma_correction(image, gamma)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 5: 17.3 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii0sdt3wTQuO"
      },
      "source": [
        "Now, we will implement the cuda kernel of the functions above,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3wYH23PTDWQ"
      },
      "source": [
        "from numba import cuda\n",
        "from operator import pow \n",
        "import numpy as np\n",
        "\n",
        "@cuda.jit\n",
        "def normalize_image_gpu(image, out):\n",
        "\n",
        "  i, j = cuda.grid(2)\n",
        "  out[i,j] = ((image[i,j])/(255)) * 10\n",
        "  \n",
        "@cuda.jit\n",
        "def gamma_correction_gpu(image, out, gamma):\n",
        "  i, j = cuda.grid(2)\n",
        "  out[i,j] = pow(image[i,j], gamma) \n",
        "  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77wU3RH6IbzI"
      },
      "source": [
        "So what we should do:\n",
        "\n",
        "1. transfer the image to device\n",
        "2. Create an intermediate arrays on device (an empty array with the same features as image array)\n",
        "3.  Create the output array on device ( for gamma correction output) with same shape as the input but with different data type (numpy.float because gamma coefficient is float)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IarTgSpoUBho"
      },
      "source": [
        "gamma = 0.5\n",
        "\n",
        "blockdim = (32, 32)\n",
        "griddim = (image.shape[0] // blockdim[0] + 1, image.shape[1] // blockdim[1] + 1)\n",
        "# data transfer to device\n",
        "d_image = cuda.to_device(image)\n",
        "d_normlized = cuda.device_array_like(image)\n",
        "d_gamma = cuda.device_array((image.shape[0], image.shape[1]), dtype=np.float)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBu9pHFg7G79",
        "outputId": "9d254761-8539-40ca-e8a6-4ef055046d68"
      },
      "source": [
        "%%timeit\n",
        "normalize_image_gpu[griddim, blockdim](d_image, d_normlized)\n",
        "gamma_correction_gpu[griddim, blockdim](d_normlized, d_gamma, gamma)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 918.07 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 282 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTZ9vyd_MSw_"
      },
      "source": [
        "Finally, here is how to retrieve the gamma correction result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yevQ4YPULy1i",
        "outputId": "466c76ec-9492-44c4-8619-3944d459e51e"
      },
      "source": [
        "gamma_result = d_gamma.copy_to_host()\n",
        "gamma_result"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.        , 2.        , 1.41421356, ..., 3.        , 2.82842712,\n",
              "        0.        ],\n",
              "       [1.73205081, 2.        , 2.82842712, ..., 2.44948974, 2.        ,\n",
              "        0.        ],\n",
              "       [2.44948974, 2.44948974, 1.73205081, ..., 1.73205081, 3.16227766,\n",
              "        2.23606798],\n",
              "       ...,\n",
              "       [1.41421356, 3.        , 1.73205081, ..., 2.82842712, 1.41421356,\n",
              "        2.82842712],\n",
              "       [1.        , 2.        , 2.44948974, ..., 1.        , 1.41421356,\n",
              "        2.        ],\n",
              "       [1.41421356, 2.64575131, 0.        , ..., 2.        , 2.64575131,\n",
              "        3.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1cyUIBhNMJT"
      },
      "source": [
        "# Device function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiDcaWBANaVF"
      },
      "source": [
        "In the examples above we have seen how to write cuda kernel of simple functions, but what if we need to write more complex function and we faced a problem where we need to call other function, what we should do? we have two solution:\n",
        "\n",
        "1. Implement everything in the same kernel and this solution seem to be bad, it made our code a Spaghetti code.\n",
        "2. Create multiple cuda kernel and call them inside other kernel, this [feature](http://developer.download.nvidia.com/GTC/PDF/GTC2012/PresentationPDF/S0338-GTC2012-CUDA-Programming-Model.pdf) is call [*dynamic parallelism*](https://developer.nvidia.com/blog/cuda-dynamic-parallelism-api-principles/), but unfortunately Numba does not support it yet. \n",
        "\n",
        "So, we need another solution, here **Device function** appear!\n",
        "\n",
        "CUDA device functions can only be invoked from within the device (by a kernel or another device function). \n",
        "Unlike a kernel function, a device function can return a value like normal functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wlyQ6a9SlW2"
      },
      "source": [
        "Let's write a device function that's similar to *numpy.max*.\n",
        "\n",
        "Note: *numpy.max* is not supported by Numba check [this](https://numba.pydata.org/numba-doc/dev/reference/numpysupported.html) to see supported NumPy features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-esSKQ7Mm_i"
      },
      "source": [
        "@cuda.jit(device=True)\n",
        "def maximum_device(arr):\n",
        "  max = arr[0,0]\n",
        "  for i in range(arr.shape[0]):\n",
        "    for j in range(arr.shape[1]):\n",
        "      if arr[i,j]>=max:\n",
        "        max = arr[i,j]\n",
        "  return max"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "VFdFzGTfUFIs",
        "outputId": "14ad2426-f0a2-46ba-817d-c2320dd30d85"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "n = 512\n",
        "\n",
        "image = np.random.randint(256, size=(n, n))\n",
        "\n",
        "maximum_device(image)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-167862ce5244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmaximum_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'DeviceFunctionTemplate' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKfTINcyUgZH"
      },
      "source": [
        "Ah yes I forget! Device function should only be declared inside CUDA kernel. We will modify our Gamma Correction Kernel!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yuK3028UVBX"
      },
      "source": [
        "@cuda.jit\n",
        "def gamma_correction_device(image, out, gamma):\n",
        "  i, j = cuda.grid(2)\n",
        "  maximum = maximum_device(image)\n",
        "  out[i,j] = pow(image[i,j], gamma) / maximum\n",
        "\n",
        "gamma = 0.5\n",
        "\n",
        "blockdim = (32, 32)\n",
        "griddim = (image.shape[0] // blockdim[0] + 1, image.shape[1] // blockdim[1] + 1)\n",
        "# data transfer to device\n",
        "d_image = cuda.to_device(image)\n",
        "d_normlized = cuda.device_array_like(image)\n",
        "d_gamma = cuda.device_array((image.shape[0], image.shape[1]), dtype=np.float)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqD9YkxyViUg",
        "outputId": "c94ec282-643b-4fb8-cf4b-e4cc5e1c755f"
      },
      "source": [
        "%%timeit\n",
        "normalize_image_gpu[griddim, blockdim](d_image, d_normlized)\n",
        "gamma_correction_device[griddim, blockdim](d_normlized, d_gamma, gamma)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 1536.68 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 289 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33lPnuTSWPMH"
      },
      "source": [
        "As we can see device function does not affect on the execution time.\n",
        "\n",
        "The device function allows us to avoid code duplication.\n",
        "\n",
        "Obviously, it is a bit artifial to use a device function in such an easy case. But when implementing complex algorithms, these functions can prove very useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4JUBY5KVonL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}